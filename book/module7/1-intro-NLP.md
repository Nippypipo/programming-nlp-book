# ข้อมูลและภาษาธรรมชาติ
ข้อมูลจัดเป็นทรัพยากรที่มีค่ามหาศาล ในปัจจุบันเกือบทุกบริษัท ทุกองค์กรทั้งภาครัฐและเอกชน ต่างเก็บข้อมูลต่าง ๆ ที่เกี่ยวกับการดำเนินธุรกิจ หรือบริหารงานทุกประเภท เช่น เมื่อเราเข้าร้านสะดวกซื้อ หรือห้างสรรพสินค้า พนักงานมักจะถามหาหมายเลขสมาชิก หรือเบอร์โทรศัพท์ เพื่อเก็บข้อมูลการซื้อของลูกค้าตลอดระยะเวลาที่ยังเป็นลูกค้าอยู่ สิ่งที่บริษัทต้องการได้คือข้อมูลของลูกค้า ถึงแม้ว่าจะต้องแลกมากลับการให้ส่วนลด หรือให้่ลูกค้าแลกแต้มเพื่อได้ของสมมนาคุณต่าง ๆ ข้อมูลเหล่านี้ทำให้บริษัทได้ศึกษาพฤติกรรมของลูกค้า ทำให้เลือกสินค้ามาขายได้ถูกใจลูกค้ามากขึ้น ได้ออกกิจกรรมส่งเสริมการขายได้ตรงใจลูกค้ามากขึ้น และทำให้สามารถลูกค้าออกมาได้เป็นกลุ่ม (เช่น กลุ่มที่เป็นลูกค้าใหม่ กลุ่มที่ซื้อไม่บ่อยแต่ซื้อเยอะ กลุ่มที่ซื้อสม่ำเสมอ เป็นต้น) เพื่อสามารถให้บริการลูกค้าได้ดีขึ้น ลูกค้ามีความพึงพอใจมากขึ้น และเพิ่มยอดขายได้มากขึ้น 

ข้อมูลอีกประเภทที่กำลังเป็นที่นิยมมากขึ้น คือข้อมูลตัวอักษร (text data) ซึ่งได้มาจากแพลตฟอร์มสื่อสังคมออนไลน์ แพลตฟอร์มการซื้อขายออนไลน์ หรือการทำสำรวจความคิดเห็นที่มีคำถามปลายเปิด ข้อมูลเหล่านี้มีหลากมิติกว่าข้อมูลที่เป็นเชิงปริมาณ หรือข้อมูลที่เป็นตัวเลข เนื่องจากผู้ที่ให้ข้อมูลสามารถแสดงความเห็นได้อย่างอิสระ ทำให้ได้คำตอบที่หลากหลาย เมื่อนำมาวิเคราะห์ทำให้เกิดความรู้เชิงประจักษ์ที่เอื้อต่อการดำเนินการต่อ (actionable insight) ที่สามารถนำไปปรับใช้กับองค์กรหรือธุรกิจได้ ตัวอย่างเช่น บริษัทสามารถดึงข้อมูลรีวิวความเห็นของลูกค้าที่ได้ซื้อสินค้าจากแพลตฟอร์มการซื้อขายออนไลน์ที่บริษัทไปเปิดร้านไว้ แล้วนำข้อมูลนี้ไปวิเคราะห์ว่าลูกค้าชอบอะไรหรือไม่ชอบอะไรเกี่ยวกับผลิตภัณฑ์ของเรา หรือลูกค้าชอบอะไรเกี่ยวกับผลิตภัณฑ์ที่ทำโดยบริษัทคู่แข่ง ทำให้เกิดความรู้เชิงประจักษ์ที่สามารถนำไปพัฒนาผลิตภัณฑ์ให้ตอบโจทย์ของผู้บริโภคได้ดีขึ้น และพัฒนากระบวนการการสั่งซื้อของและส่งของให้ลูกค้าให้มีประสิทธิภาพมากขึ้น

การใช้เทคโนโลยีในการวิเคราะห์ข้อมูล เพื่อสกัดความรู้เชิงประจักษ์ที่ก่อให้เกิดประโยชน์และมูลค่าทางธุรกิจ เรียกว่า วิทยาการข้อมูล (data science หรือ data analytics)  เป็นการผสมผสานระหว่างศาสตร์และความรู้การบริหารธุรกิจ ซึ่งทำให้เราเข้าใจกลไกในการประกอบธุรกิจและเข้าใจลูกค้าให้มีผลประกอบการที่ดี สถิติ ซึ่งทำให้เราสามารถวิเคราะห์และสรุปหาแพทเทิร์นในข้อมูลที่มีขนาดใหญ่ และวิทยาการคอมพิวเตอร์ (computer science) ซึ่งทำให้เราสามารถเขียนโปรแกรมที่สามารถใช้แบบจำลองที่ซับซ้อนหรือจัดการกับข้อมูลที่มีขนาดใหญ่และโครงสร้างซับซ้อนได้ แต่เมื่อเราต้องวิเคราะห์ข้อมูลตัวอักษร หรือข้อมูลที่เป็นภาษาธรรมชาติ (natural language data) ซึ่งไม่สามารถนำมาหาค่าเฉลี่ย หรือบวกลบคูณหารอย่างที่ทำกับข้อมูลเป็นเชิงปริมาณ เราจึงต้องใช้เทคนิควิธีการประมวลผลภาษาธรรมชาติ ซึ่งมีแบบจำลองในการทำความเข้าใจภาษาเพื่อการวิเคราะห์ข้อมูลเหล่านี้ เพราะฉะนั้นการประมวลผลภาษาธรรมชาติ คือ เทคโนโลยีที่ใช้ในการประมวลผลและทำความเข้าใจข้อมูลตัวอักษร โดยอาศัยแบบจำลองทางภาษา เมื่อนำมาประกอบกับการวิเคราะห์ข้อมูลเพื่อพัฒนาธุรกิจ เราจะเรียกว่าการวิเคราะห์ข้อความ (text analytics)


```{margin} คำศัพท์
วิทยาการข้อมูล (data science) คือสหศาสตร์ที่ใช้วิธีการทางสถิติ การคำนวณด้วยเครื่องคอมพิวเตอร์ แบบจำลอง และอัลกอริทึมในการสกัดความรู้เชิงประจักษ์จากข้อมูลที่อาจจะมีสิ่งรบกวน หรือจัดเก็บอย่างไม่เป็นระเบียบ
```

นอกจากนั้นการประมวลผลภาษาธรรมชาติ เป็นเทคโนโลยีที่เป็นสันหลังของแอพพลิเคชันที่ทำหน้าที่ทางภาษาโดยอัตโนมัติได้ ตัวอย่างเช่น Google Translate เป็นแอพพลิเคชันทำหน้าที่แปลภาษาโดยอาศัยแบบจำลองทางภาษาที่เข้าใจทั้งภาษาต้นทางที่ต้องการแปลและภาษาปลายทาง หรือแอพพลิเคชัน ChatGPT ที่สามารถทำหน้าที่ทางภาษาได้อย่างหลากหลาย ไม่ว่าจะเป็นการสรุปข่าว การแต่งนิยาย การปรับแก้ภาษาให้สละสลวยไร้ข้อผิดพลาด การตอบคำถามที่เป็นปลายเปิด การให้คำปรึกษาเรื่องต่าง ๆ หรือแอพพลินเคชัน Google Search เองที่สามารถทำความเข้าใจสิ่งที่ผู้ใช้ต้องการค้นหา โดยพิจารณาจากคำค้นที่ผู้ใช้พิมพ์เข้ามาในกล่อง และทำความเข้าใจเว็บไซต์ทุกเว็บไซต์ และเลือกมาเฉพาะเว็บไซต์ที่ตอบสนองโจทย์ความต้องการทางข้อมูลของผู้ใช้ตามที่ได้ระบุมาในคำค้น

สรุปคือการประยุกต์ใช้ NLP สามารถนำไปใช้ประโยชน์ได้อย่างน้อย 2 ทาง คือ 
1. เครื่องมือ text analytics ที่สกัดความรู้เชิงประจักษ์
2. เทคโนโลยีหลังบ้านของแอพพลิเคชันที่ทำหน้าที่ทางภาษาโดยอัตโนมัติ 


## หลักการของการประมวลผลภาษาธรรมชาติ
ข้อมูลตัวอักษรมักจะเก็บอยู่ในรูปของสตริง หรือเก็บอยู่ในโครงสร้างข้อมูลที่เก็บสตริงอยู่ เช่น ลิสต์ของสตริง ข้อมูลเมื่อรวบรวมมาอยู่ในชุดเดียวกัน เราเรียกว่าชุดข้อมูล (dataset) เช่น ชุดข้อมูลทวิตเตอร์ที่เก็บมาจากแฮชแท็กหนึ่งจากช่วงเวลาหนึ่ง ชุดข้อมูลข่าวต่างประเทศจากหนังสือพิมพ์ไทยออนไลน์จากช่วงเวลาหนึ่ง เป็นต้น ชุดข้อมูลชุดหนึ่งประกอบด้วย ข้อมูลหลาย ๆ แถว (row) หรือเรียกอีกอย่างหนึ่งได้ว่า ระเบียน หรือเรคคอร์ด (record) เช่น ชุดข้อมูลทวิตเตอร์มีข้อมูลอยู่ 50,000 แถว ซึ่งก็คือ 50,000 ทวีต หรือชุดข้อมูลข่าวมีข้อมูลอยู่ 10,000 แถว ซึ่งก็คือ 10,000 บทความ 

```{margin} คำศัพท์
ชุดข้อมูล (dataset) คือ ชุดของข้อมูลที่มาจากแหล่งเดียวกัน หรือมีลักษณะอื่น ๆ คล้ายกัน และถูกจัดเก็บอยู่ในลักษณะที่พอจะใช้เครื่องในการประมวลผลได้ 

แถว (row) คือ ข้อมูลหน่วยหนึ่งในชุดข้อมูล
```

ข้อมูลแต่ละแถวที่อยู่ในชุดข้อมูลเป็นเพียงสตริง ซึ่งตัวสตริงเองนั้นไม่ได้มีความหมายอะไรในตัวมันเอง  เป็นเพียงรูปแบบการเก็บข้อมูลในรูปแบบดิจิทัลที่นำตัวอักษรมาร้อยเรียงกัน เราจึงเรียกข้อมูลตัวอักษรว่า ข้อมูลแบบไม่มีโครงสร้าง (unstructured data)  การที่จะทำให้เครื่องคอมพิวเตอร์สามารถเข้าใจความหมายได้จำเป็นต้องใช้ทฤษฎีทางด้านภาษาศาสตร์เข้ามาช่วยทำให้สตริงมีโครงสร้างมากขึ้น 

ภาษาศาสตร์ เป็นศาสตร์ที่วิเคราะห์ภาษาออกเป็นโครงสร้างย่อย ๆ เช่น ประโยค กลุ่มคำ คำ พยางค์ เสียงพยัญชนะ เสียงสระ หน่วยคำ เพื่อโยงโครงสร้างต่าง ๆ เข้ากับลักษณะทางภาษาทุกด้าน การประยุกต์ใช้ NLP อาศัยการวิเคราะห์ส่วนย่อย ๆ ของภาษา และโครงสร้างของภาษากับความหมาย เช่น ถ้าหากเราต้องการทราบว่าข้อมูลทวิตเตอร์ที่ติดแฮชแท็กชื่อสินค้าของบริษัทเรา พูดถึงสินค้าเราในแง่บวกหรือลบ โปรแกรมอาจจะต้องตรวจหาว่า 

- คำใดบ้างที่ใช้ในการพูดถึงสินค้าของเรา หรือสินค้าของคู่แข่ง
- คำใดบ้าง และกลุ่มคำใดบ้างที่ใช้ในการสื่อความหมายในแง่บวก แง่ลบ
- คำใดบ้างที่ใช้เพื่อบ่งบอกว่าข้อความนั้นไม่ได้มีความคิดเห็นแฝงอยู่ แต่อาจจะเป็นการให้ข้อมูลอย่างเป็นกลางเท่านั้น 
- ลักษณะประโยคแบบใดที่แสดงให้เห็นถึงน้ำเสียงแบบประชดประชัน หรือล้อเล่น 
- การรีทวีทตอบโต้กันระหว่างผู้ใช้บนแพลตฟอร์ม แสดงถึงความคิดเห็นของลูกค้าต่อสินค้าของเราอย่างไร 
- ลักษณะทางภาษาใดบ้าง ทำให้เราทราบถึงอายุ เพศ ถิ่นที่อยู่ของลูกค้าได้

## การทำความสะอาดข้อมูล (Data cleaning)

ขั้นตอนแรกของการประมวลผลข้อมูล คือ การทำความสะอาดข้อมูล ข้อมูลที่เราได้รับมามักจะไม่สะอาด มีรอยเปื้อน มีความผิดปกติอยู่ ถ้าหากเราไม่ทำความสะอาดดี ๆ ให้มีความเพี้ยนน้อยลง ให้เหลือเฉพาะส่วนที่เราต้องการวิเคราะห์ การทำความสะอาดข้อมูลไม่ได้มีสูตรสำเร็จตายตัว เราต้องปรับกระบวนการให้เข้ากับจุดประสงค์ของการวิเคราะห์ ดังกรณีตัวอย่างต่อไปนี้

### ตัวอย่างการทำความสะอาดข้อมูล 1 

> RT @MatichonOnline: “บิ๊กตู่”ลั่นรบ.ทำอะไรยึดกม.ไม่ใช่ติดคุกแล้วหนี เล่นมุกพรรคร่วม “พลังปชป.ภูมิใจไทย” https://t.co/9nmOBJnhrq via @มติชนอ…

ตัวอย่างข้างบนนี้มีข้อมูลหลายส่วนที่ไม่ใช่ข้อความที่เราต้องการวิเคราะห์ ได้แก่ 

- RT @MatichonOnline ซึ่งหมายถึงการรีทวิตข่าวจาก MatichonOnline
- ลิงก์ https://t.co/9nmOBJnhrq ซึ่งเป็นลิงก์ที่เชื่อมไปยังเว็บไซต์ที่นำเสนอข่าวฉบับเต็มอยู่
- via @มติชนอ… ซึ่งเป็นการบอกว่าลิงก์ที่นำไปสู่เว็บไซนต์ของมติชน

เราจึงจำเป็นต้องเขียนโค้ดเพื่อใช้ regular expression ในการสกัดเอาข้อมูลส่วนที่เราไม่ต้องการออกไป ให้เหลือเพียงแค่ 

> “บิ๊กตู่”ลั่นรบ.ทำอะไรยึดกม.ไม่ใช่ติดคุกแล้วหนี เล่นมุกพรรคร่วม “พลังปชป.ภูมิใจไทย”

ถ้าเราเก็บข้อมูลที่ยังไม่ได้ทำความสะอาดใส่ตัวแปรชื่อว่า `tweet` เราสามารถเขียนโค้ดในการทำความสะอาดได้ดังนี้
```python
import re
tweet = 'RT @MatichonOnline: “บิ๊กตู่”ลั่นรบ.ทำอะไรยึดกม.ไม่ใช่ติดคุกแล้วหนี เล่นมุกพรรคร่วม “พลังปชป.ภูมิใจไทย” https://t.co/9nmOBJnhrq via @มติชนอ…'
# Remove RT @username
tweet = re.sub(r'RT @\w+: ', '', tweet)
# Remove URL that begins with http
tweet = re.sub(r'https?://\S+', '', tweet)
# Remove via @username
tweet = re.sub(r' via @\S+', '', tweet)
```
ซึ่งเราอาจจะรวมเป็นฟังก์ชันที่ทำให้เราใช้งานกับทวีตอื่น ๆ ได้ด้วย ดังนี้
```python
def clean_tweet(tweet):
    # Remove RT @username
    tweet = re.sub(r'RT @\w+: ', '', tweet)
    # Remove URL that begins with http
    tweet = re.sub(r'https?://\S+', '', tweet)
    # Remove via @username
    tweet = re.sub(r' via @\S+', '', tweet)
    return tweet
```

### ตัวอย่างการทำความสะอาดข้อมูล 2

> บกพร่องโดยสุจริต VS อยู่-ไม่-เป็น | ขยี้คดีโกง | 10 พ.ย. 62 | (3/3) https://t.co/atUF6PrXdx via @YouTube

ตัวอย่างข้างต้นนี้มีข้อมูลที่เราไม่ต้องการอยู่ด้วยหลายส่วน ได้แก่ 
- (3/3) ซึ่งหมายถึง ทวีตนี้เป็นทวีตที่ 3 ในชุดทวีตทั้งหมด 3 ทวีต
- ลิงก์ https://t.co/atUF6PrXdx ซึ่งเป็นลิงก์ที่เชื่อมไปยังวีดีโอที่อยู่บน YouTube
- via @YouTube ซึ่งเป็นการบอกว่าลิงก์ที่นำไปสู่วีดีโอนั้นอยู่บนแพลตฟอร์ม YouTube

ในกรณีนี้จะเห็นว่าข้อมูลที่เราได้มาไม่สมบูรณ์ เนื่องจากเป็นทวีีตเป็นทวีตต่อเนื่องจากสองทวีตก่อนหน้า ผู้วิเคราะห์อาจจะเลือกไม่วิเคราะห์ทวีตที่มีความต่อเนื่องกันในลักษณะนี้ หรือไม่่เช่นนั้นต้องเตรียมข้อมูลให้เรียงลำดับตามการทวีตและเชื่อมทวีตเข้าไว้ด้วยกัน เช่น สมมติว่าเรามีข้อมูลลิสต์ของทวีต เราสามารถเขียนโค้ดเพื่อรวมทวีตที่มีการต่อเนื่องกัน เอาไว้ด้วยกันได้ดังนี้

```python
def merge_tweet_in_sequence(tweet_list_time_sorted):
    new_list = [] 
    index = 0
    while (index < len(tweet_list_time_sorted)):
        # if tweet contains (1/x), merge this tweet with the next x tweets
        patt = re.compile(r'\(1/(\d)\)')
        tweet = tweet_list_time_sorted[index]
        match = patt.search(tweet)
        if match:
            num_tweets = int(match.group(1))
            merged_tweet = clean_tweet(tweet)
            for i in range(1, num_tweets):
                merged_tweet += clean_tweet(tweet_list_time_sorted[index + i])
            new_list.append(merged_tweet)
            index += num_tweets
        else:
            new_list.append(tweet_list_time_sorted[index])
            index += 1
    return merged_tweet
```

### ตัวอย่างการทำความสะอาดข้อมูล 3

> ขอไห้นึกถึงการท่องเที่ยวภายภาคหน้าด้วยคะ

ตัวอย่างข้างต้นนี้มีการสะกดผิดสองจุด ได้แก่

- การใช้คำว่า ขอไห้ แทนคำว่า ขอให้
- การใช้คำว่า คะ แทนคำว่า ค่ะ

โดยทั่วไปแล้วเรามักจะไม่แก้การสะกดผิด เพราะส่วนใหญ่แล้วเรามักจะวิเคราะห์ข้อมูลที่มีขนาดค่อนข้างใหญ่ ใหญ่เกินที่จะให้มนุษย์วิเคราะห์เองได้ทันท่วงที วิธีทางสถิติหรือการใช้โมเดลทางภาษาอาศัยการจับแพทเทิร์นต่าง ๆ ที่อยู่ในข้อมูล การสะกดผิดตามสถิติแล้วมักจะเกิดขึ้นไม่มาก เมื่อเทียบกับส่วนของข้อมูลที่สะกดถูกตามหลักพจนานุกรม หรือตามความนิยมในช่วงเวลานั้น ทำให้ส่วนใหญ่แล้วมักจะไม่ต้องกังวลว่าการวิเคราะห์จะผิดเพี้ยนเนื่องจากมีคำที่สะกดผิดอยู่
อีกเหตุผลหนึ่งที่เรามักจะตัดสินใจไม่แก้ไขคำที่สะกดผิดก่อนวิเคราะห์ข้อมูล คือ เราไม่มีโปรแกรมที่สามารถแก้ไขการสะกดผิดได้อย่างแม่นยำพอ เครื่องตรวจตัวสะกด (spellchecker) มีความแม่นยำระดับหนึ่ง แต่ก็มีโอกาสที่จะแก้ไขส่วนที่ผิดให้ผิดไปอีกแบบหนึ่ง หรือเปลี่ยนส่วนที่ถูกอยู่แล้วให้เป็นผิด  และเมื่อการวิเคราะห์ออกมาแปลกหรือผิดเพี้ยนจากที่สิ่งที่เราคาดการณ์ไว้ ทำให้เกิดเป็นอีกขั้นตอนหนึ่งที่เราต้องมาตรวจสอบว่า ความผิดเพี้ยนนั้นเกิดจากเครื่องตรวจตัวสะกดที่เราเลือกมาใช้หรือไม่ 

### ตัวอย่างการทำความสะอาดข้อมูล 4

> ชาล็อตพาร์ทแบบนี้จึ้งเว่อ น้องเก่งมาก มืออาชีพสุดๆ ขนาดพี่ในกองส่งเสียงชมไม่หยุด สวยมาก ดีมากกกกกก 
#ENGLOTshootingTvcWinkwhite
@itscharlotty
(ที่มา: Twitter @vanitcheryl วันที่ 2 มีนาคม 2567)

ในโลกโซเชียลเรามักจะพบภาษาไม่ได้เป็นไปตามมาตรฐาน ในตัวอย่างนี้เราพบลักษณะของภาษาโซเชียลหลายจุด ได้แก่

- การใช้ไทยคำอังกฤษคำ เช่น *พาร์ท* 
- การใช้คำแสลง เช่น *จึ้ง* *เว่อ*
- การสะกดแบบไม่มาตรฐาน เช่น *มากกกกกก*

ตามหลักภาษาศาสตร์แล้ว ภาษาที่เป็นมาตรฐานเป็นภาษาที่มีคนกลุ่มใดกลุ่มหนึ่งในสังคมเป็นคนกำหนดมาว่าเป็นมาตรฐาน คนกลุ่มนั้นมักจะเป็นกลุ่มคนที่มีสถานะทางสังคมและเศรษฐกิจสูง คนกลุ่มนี้มักจะกระทำการใด ๆ ให้สังคมยอมรับว่ารูปแบบภาษาที่ตนตั้งขึ้นมานั้นเป็นภาษาสำหรับคนที่ได้รับการยอมรับนับถือจากสังคมด้วยวิธีต่าง ๆ ในมุมมองของนักภาษาศาสตร์เห็นว่าเป็นการปะทะสังสรรค์ระหว่างปัจจัยทางสังคม และรูปแบบของภาษา การศึกษาภาษาในบริบทของสังคมและวัฒนธรรม เรียกว่า ภาษาศาสตร์สังคม (sociolinguistics) จากแง่มุมนี้ภาษาเป็นเพียงเครื่องมือที่ใช้ในการสื่อสาร และทุกภาษาต่างมีแบบแผนของตัวมันเอง การตีค่าว่ารูปแบบภาษาใด ภาษาหนึ่ง หรือภาษาใด ภาษาหนึ่งนั้นดีกว่าอีกภาษาหนึ่งนั้น ล้วนแต่เป็นสิ่งที่ถูกสร้างขึ้นโดยสังคม (social construct)

```{margin} คำศัพท์
ภาษาศาสตร์สังคม (sociolinguistics) คือการศึกษาเกี่ยวกับผลกระทบของปัจจัยทางสังคม (เช่น บรรทัดฐาน สถานะทางสังคม และบริบททางวัฒนธรรม) ต่อวิธีการใช้ภาษา 
``` 

หากเราต้องการวิเคราะห์ข้อมูลที่มาจากโลกโซเชียลมักจะไม่มีความจำเป็นใด ๆ ที่จะต้องแก้ภาษาให้เป็นมาตรฐาน เนื่องจากภาษาในโลกโซเชียลปรากฏเป็นแพทเทิร์นของมันเอง วิธีทางสถิติและแบบจำลองทางภาษาอาศัยการหาแพทเทิร์นที่เกิดขึ้นซ้ำ ๆ อยู่แล้ว เช่น คำว่า *จึ้ง* ปรากฏอยู่ถึง 6.7 ล้านครั้ง และ คำว่า *เว่อ* ปรากฏอยู่ถึง 18.7 ล้านครั้ง เมื่อลองค้นหาคำเหล่านี้บนกูเกิ้ล (วันที่ 2 มีนาคม 2567) ซึ่งแสดงให้เห็นว่าสองคำนี้กลายเป็นส่วนหนึ่งของภาษาแล้ว สังคมบนโลกอินเตอร์เน็ตยอมรับและนำไปใช้ต่ออย่างแพร่หลายในขณะนั้น 

ส่วนคำว่า *มากกกกกก* เราอาจจะทำความสะอาดให้เหลือ ก เพียงตัวเดียว เพราะคนแต่ละคนอาจจะใช้จำนวนตัว ก ไม่เท่ากัน ทำให้เครื่องมือตรวจจับได้ยากว่าใช้คำว่า *มาก* ไปแล้วกี่ครั้ง กระบวนการนี้เราเรียกว่าการเปลี่ยนให้เป็นมาตรฐาน (normalization) สำหรับภาษาไทยเรามักจะใช้กฎง่าย ๆ ในการเปลี่ยนให้เป็นมาตรฐาน โดยการตรวจจับว่ามีตัวอักษรเดียวกัน ตั้งแต่สามตัวขึ้นไปหรือไม่ ถ้ามีให้ทำให้เหลือตัวเดียว ซึ่งสามารถทำโดยใช้ regular expression ดังตัวอย่างโค้ดดังนี้

```python
import re 
def normalize(tweet):
    # ถ้าเจอ [ก-์] สามตัวขึ้นไป ทำให้เหลือตัวเดียว
    tweet = re.sub(r'([ก-์])\1{2,}', r'\1', tweet)
    return tweet
```
คำอธิบาย regular expression ที่ใช้ในโค้ดด้านบนคือ
- `([ก-์])` หมายถึง ตัวอักษรไทยที่อยู่ในช่วง ก ถึง ์ และเก็บไว้ในกลุ่มที่ 1 (เครื่องหมายวงเล็บคู่แรก)
- `\1{2,}` หมายถึง อ้างกลับถึงตัวอักษรที่อยู่ในกลุ่มที่หนึ่ง (`\1`) ตรวจว่าเจอตั้งแต่ 2 ตัวขึ้นไปหรือไม่ (`{2,}`)
- `r'\1'` หมายถึง ตัวอักษรที่ตรวจพบและเก็บอยู่ในกลุ่มที่ 1 

ตัวอย่างการใช้
```python
normalize('คิดถึงงงงงมากกกกก') # คิดถึงมาก
```

ขั้นตอนสุดท้ายของการทำความสะอาดข้อมูลคือ การกำจัดตัวซ้ำ (deduplication) โดยทั่วไปแล้วข้อมูลแต่ละชิ้นมักจะไม่ซ้ำกัน ชุดข้อมูลทีมาจากทวิตเตอร์อาจจะมีซ้ำกันบ้าง เมื่อผู้ใช้รีทวีตผู้ใช้อีกคนหนึ่ง เมื่อนำข้อความ *RT @* ตามด้วยชื่อผู้ใช้ออกไปแล้ว ก็จะเหลือเพียงข้อความที่เหมือนกับเจ้าของทวีต ถ้าในชุดข้อมูลมีทวีตที่ถูกรีทวีตบ่อย ๆ ก่อให้เกิดแถวที่มีข้อมูลซ้ำ ๆ กันมากมาย ทำให้ค่าสถิติของคำถูกบิดเบือนไป เพราะฉะนั้นเราจึงจำเป็นต้องนำข้อมูลที่ซ้ำออกไป วิธีการที่ง่ายที่สุดคือใช้เซตในการเก็บข้อมูลที่เคยเจอแล้ว ดังนี้

```python
def deduplicate(tweet_list):
    seen = set()
    deduplicated_tweet_list = []
    for tweet in tweet_list:
        if tweet not in seen:
            deduplicated_tweet_list.append(tweet)
            seen.add(tweet)
    return deduplicated_tweet_list
```

```{margin} คำศัพท์
การกำจัดตัวซ้ำ (deduplication) คือ การตรวจหาแถวที่มีข้อมูลซ้ำกันแล้วลบทิ้งไป ให้เหลือเพียงแค่แถวที่ไม่ซ้ำกันเท่านั้น 
```

เมื่อรวมโค้ดทั้งหมดเข้าด้วยกัน จะได้ฟังก์ชันที่ทำความสะอาดข้อมูลทวีตได้ดังนี้

```python
cleaned_normalized_tweets = []
for tweet in tweet_list:
    tweet = clean_tweet(tweet)
    tweet = normalize(tweet)
    cleaned_normalized_tweets.append(tweet)
dataset = deduplicate(cleaned_normalized_tweets)
```
หรืออาจจะรวมกันเป็นฟังก์ชันเดียวกันได้ดังนี้

```python
def clean_normalize_tweet(tweet):
    tweet = clean_tweet(tweet)
    tweet = normalize(tweet)
    return tweet

merged_tweet_list = merge_tweet_in_sequence(tweet_list)
cleaned_normalized_tweets = [clean_normalize_tweet(tweet) for tweet in merged_tweet_list]
dataset = deduplicate(cleaned_normalized_tweets)
```
ทั้งนี้เราจะเห็นว่าเราต้องใช้ 4 ฟังก์ชันในการประมวลผลทำความสะอาดข้อมูล `merge_tweet_in_sequence`, `clean_tweet`, `normalize`, และ `deduplicate`   
ซึ่งการแยกออกมาเป็น 4 ฟังก์ชันนั้นมีข้อดีคือทำให้เราปรับแก้เป็นส่วน ๆ ไปได้ ถ้าหากว่าเราต้องการปรับวิธีการทำความสะอาดข้อมูลให้นำเอาแฮชแท็กออกไปด้วย เราสามารถเพิ่มขั้นตอนนี้เข้าไปใน `clean_tweet` หรือเขียนฟังก์ชัน `remove_hashtag` และเรียกใช้ในฟังก์ชัน `clean_tweet` อีกชั้นหนึ่งก็ได้ ขึ้นอยู่กับการตัดสินใจของผู้เขียนโปรแกรมว่าแบบใดเข้าใจง่ายกว่า 

## การแปลงให้เป็นโทเค็น (tokenization)

ขั้นตอนต่อจากการทำความสะอาดข้อมูล คือ การแปลงให้เป็นโทเค็น (tokenization) กระบวนการนี้เป็นกระบวนพิเศษสำหรับการเตรียมข้อมูลจากชุดข้อมูลที่เป็นตัวอักษร 
โทเค็น คือ หน่วยที่เล็กที่สุดที่ใช้ในการวิเคราะห์ หน่วยที่เล็กที่สุดที่ใช้ในการวิเคราะห์ข้อมูลตัวอักษรคืออะไร โดยทั่วไปแล้วผู้วิเคราะห์ข้อมูลสามารถกำหนดได้ว่าอยากให้โทเค็นเป็นอะไร ส่วนมากเราจะอ้างอิงหลักการการวิเคราะห์ภาษาจากภาษาศาสตร์ คือใช้คำเป็นโทเค็นในการวิเคราะห์ความหมายของประโยค เพราะฉะนั้นการวิเคราะห์ความหมายของข้อความมักจะต้องอาศัยการเปลี่ยนสตริงให้เป็นลิสต์ของคำ เรียกว่า การตัดคำ (word segmentation) บางครั้งเราเรียกกระบวนการแปลงให้เป็นโทเค็น ว่าการตัดคำ ถึงแม้ที่จริงแล้วการแปลงให้เป็นโทเค็นเป็นมโนทัศน์ที่กว้างกว่าการตัดคำ  เพราะเราสามารถกำหนดให้โทเค็นเป็นอะไรก็ได้ ไม่จำเป็นต้องเป็นคำเท่านั้น

คำ ในเชิงภาษาศาสตร์ คำ คือ หน่วยที่เล็กที่สุดของภาษาที่ยังคงสื่อความหมายได้ด้วยตัวเอง  ตัวอย่างเช่น 

| ข้อความ | เป็นคำหรือไม่ | เหตุผล |
|---------|-----------------|---------|
| สวัสดีครับ | ไม่ใช่ | เป็นกลุ่มคำที่มีสองคำอยู่ *สวัสดี* และ *ครับ* ต่างเป็นคำที่สื่อความหมายด้วยตัวเอง|
| ตัดหญ้า | ไม่ใช่ | เป็นกลุ่มคำที่มีสองคำอยู่ *ตัด* และ *หญ้า* ต่างเป็นคำที่สื่อความหมายด้วยตัวเอง ความหมายของ *ตัดหญ้า* มาจากการรวมความหมายของคำสองคำ |
| ตัดใจ | ใช่ | เป็นคำที่สื่อความหมายด้วยตัวเอง และไม่ใช่การประกอบความหมายของ *ตัด* และ *ใจ* | 
| คิดถึง | ใช่ | เป็นคำเดียวกันที่สื่อความหมายด้วยตัวเอง และไม่ใช่การประกอบความหมายของ *คิด* และ *ถึง* |
| เดินทาง | ใช่ | เป็นคำเดียวกันที่สื่อความหมายด้วยตัวเอง และไม่ใช่การประกอบความหมายของ *เดิน* และ *ทาง* |

จากตัวอย่างข้างต้น จะเห็นได้ว่าการตัดคำต้องอาศัยเกณฑ์ทางความหมาย เพื่อตัดสินว่าสตริงที่มีสตริงย่อยเป็นคำ (เช่น *ตัด|ใจ* หรือ *ตัด|หญ้า*)
เป็นกลุ่มคำ หรือคำเดี่ยว  หากว่าความหมายไม่ได้ต่างจากการนำคำย่อยมารวมกันให้จัดว่าเป็นกลุ่มคำ ตัวอย่างเช่น คำว่า *ตัดหญ้า* มีความหมายจากคำกริยา *ตัด* รวมกับคำนามที่เป็นกรรม *หญ้า* มารวมกัน ดังนั้นเราจึงจัดเป็นคำสองคำมาอยู่ใกล้กันเป็นกลุ่มคำ ในขณะที่คำว่า *ตัดใจ* มีความหมายว่า เลิกคิด  และไม่ได้มาจากการนำคำย่อย *ตัด* และ *ใจ* มารวมกัน ดังนั้นเราจึงจัดเป็นคำเดี่ยว {cite}`aroonmanakun2007thoughts`

`````{admonition} คำถามชวนคิด
:class: note

จากกรณีให้ลองคิดว่ากรณีใดบ้างที่เป็นคำ กรณีใดบ้างที่เป็นกลุ่มคำ 

- *ตู้เย็น*
- *ตะกร้าผ้า*
- *ขวดแก้ว*
- *หมอฟัน* 

*เฉลย*
- *ตู้เย็น* เป็นคำเดี่ยว เพราะว่าความหมายคือ เครื่องใช้ไฟฟ้าที่ทำความเย็น ซึ่งห่างจากความหมายของคำย่อย *ตู้* และ *เย็น*
- *ตะกร้าผ้า* เป็นกลุ่มคำ เพราะว่าความหมายคือ ตะกร้าที่ใช้ใส่ผ้า ซึ่งมาจากการนำคำย่อย *ตะกร้า* และ *ผ้า* มารวมกัน
- *ขวดแก้ว* เป็นกลุ่มคำ เพราะว่าความหมายคือ ขวดที่ทำจากแก้ว ซึ่งมาจากการนำคำย่อย *ขวด* และ *แก้ว* มารวมกัน
- *หมอฟัน* เป็นกลุ่มคำ เพราะว่าความหมายคือ ผู้เชี่ยวชาญที่ทำงานเฉพาะทางที่เกี่ยวกับฟัน ซึ่งมาจากการนำคำย่อย *หมอ* และ *ฟัน* มารวมกัน สามารถใช้หลักคิดนี้ได้กับ *หมอตา* *หมอผี* *หมอกระดูก*

``````

ในบางกรณีการพิจารณาว่าอะไรจัดเป็นคำ อะไรจัดเป็นกลุ่มคำ เป็นเรื่องที่ไม่ชัดเจนสักทีเดียว อาจจะทำให้เกิดการถกเถียงว่าความหมายโดยรวมเกิดจากการนำความหมายของสองคำย่อยมารวมกันหรือไม่ 
ในกรณีดังกล่าว เรามักจะตัดคำให้มีจำนวนคำมากที่สุด เพราะถ้าหากหลักการวิเคราะห์เปลี่ยนไปเรายังสามารถนำคำที่ตัดไปแล้วมารวมกันใหม่ได้ 

การตัดคำโดยอัตโนมัติสามารถทำด้วย 3 วิธีใหญ่ ๆ ได้แก่ 
1. การตัดคำโดยอาศัยกฎเกณฑ์ (rule-based word segmentation) 
2. การตัดคำโดยอาศัยคลังศัพท์ (lexicon-based word segmentation) 
3. การตัดคำโดยอาศัยการเรียนรู้ด้วยเครื่อง (machine-learning-based word segmentation)

### การตัดคำโดยอาศัยกฎเกณฑ์ (rule-based word segmentation)

บางภาษาเราสามารถตั้งกฏเกณฑ์ผ่านการเขียน regular expression เพื่อตัดคำได้ เช่น ภาษาอังกฤษ ภาษาเยอรมัน ภาษาอิตาเลียน และภาษาอื่น ๆ ที่ใช้ตัวอักษรลาติน กฏเกณฑ์ที่ตั้งอาจจะเป็น regular expression เพื่อบอกว่าจะแพทเทิร์นไหนเป็นตัวแบ่งบ้าง เช่น เราอาจจะใช้เครื่องหมายวรรคตอน และช่องว่างเป็นตัวแบ่ง ซึ่งตรงกับ regular expression `r'\s+'` เพราะว่า `\s` หมายถึงตัวอักษรที่เป็น whitespace ได้แก่ ช่องว่าง แท็บ และการขึ้นบรรทัดใหม่ และเราเผื่อว่ามีการใช้แท็บหลาย ๆ ครั้ง หรือขึ้นบรรทัดใหม่หลาย ๆ ครั้ง เมื่อเราอ่านเอาข้อมูลมาจากไฟล์ที่มีหลายบรรทัด ยกตัวอย่างเช่น 

```python
import re
text = "Got a long list of ex-lovers, they'll tell you I'm insane (Yeah) "
tokens = re.split(r'\s+', text)
print(tokens)
```
ได้ผลลัพธ์เป็น
```python
['Got', 'a', 'long', 'list', 'of', 'ex-lovers,', "they'll", 'tell', 'you', "I'm", 'insane', '(Yeah)', '']
```
เราสังเกตเห็นได้ว่าโทเค็นที่ถูกต้องมีทั้งหมด 8 ตัว และผิด 4 ตัว ได้แก่ *ex-lovers,* *they'll* *I'm* และ *(Yeah)* เพราะว่าเครื่องหมายวรรคตอนไปติดอยู่กับคำ ใช้ `re.split` เป็นวิธีง่ายเหมาะกับการวิเคราะห์ข้อมูลเบื้องต้น ไม่ต้องการความละเอียดมาก แต่ว่าถูกต้องประมาณ 70% เท่านั้นสำหรับภาษาอังกฤษ 

อีกวิธีที่นิยมในการตัดคำ คือ การเขียน regular expression เพื่อบอกแพทเทิร์นของโทเค็น แทนที่จะเขียน regular expression เพื่อบอกแพทเทิร์นของตัวแบ่ง สำหรับภาษาที่ใช้ตัวอักษรลาติน หรือภาษาอื่น ๆ ที่มีการใช้ตัวแบ่งระหว่างคำชัดเจน เรามักจะใช้ regular expression `\w+|[^\w\s]+` ซึ่งประกอบด้วยสองแพทเทิร์นย่อย ได้แก่

- `\w+` เราต้องการโทเค็นที่เป็น ตัวเลขหรือตัวอักษร a-z (ตัวลาติน) รวมถึงตัวอักษรที่มีเครื่องหมายแสดงการออกเสียงที่อยู่บนหรือล่างตัวอักษร (diacritics) เช่น é หรือ ä หรือตัวอักษรจากระบบการเขียนอื่น ๆ ที่จัดว่าเป็นตัวหนังสือ ไม่ใช่เครื่องหมายวรรคตอน 
- `[^\w\s]+` เราต้องการโทเค็นที่เป็น เครื่องหมายวรรคตอนล้วนหรือเครื่องหมายอื่น ๆ ที่ไม่ใช่ตัวเลขหรือตัวอักษร โทเค็นนี้จะมีแต่เครื่องหมายวรรคตอนไม่มีตัวอักษรปนอยู่ เพื่อที่จะแยก `ex-lovers` เป็น `['ex', '-', 'lovers']` และ `they'll` เป็น `['they', "'", 'll']` และ `(Yeah)` เป็น `['(', 'Yeah', ')']`

ตัวอย่างการใช้ 

```python
import re
text = "Got a long list of ex-lovers, they'll tell you I'm insane (Yeah) "
tokenizing_pattern = r'\w+|[^\w\s]+'
tokens = re.findall(tokenizing_pattern, text)
print(tokens)
```
ผลลัพธ์เป็น
```python
['Got', 'a', 'long', 'list', 'of', 'ex', '-', 'lovers', ',', 'they', "'", 'll', 'tell', 'you', 'I', "'", 'm', 'insane', '(', 'Yeah', ')']
```
เราสังเกตเห็นได้ว่าโทเค็นที่ถูกต้องมีทั้งหมด 17 ตัว และผิด 4 ตัว ได้แก่ *'ll* และ *'m* ซึ่งผลลัพธ์ที่ได้จากการใช้ `re.findall` มีความถูกต้องมากกว่าการใช้ `re.split` แต่ก็ยังไม่ถูกต้อง 100% สำหรับภาษาอังกฤษ และยังมีแพทเทิร์นอื่นอีกไม่สามารถเขียน regular expression มาตัดได้ง่าย ๆ เช่น

- ตัวเลขที่เป็นจุดทศนิยม
- 's ที่ใช้แสดงความเป็นเจ้าของ เช่น *John's car*
- อักษรย่อ เช่น *U.S.A.*

การตัดคำโดยอาศัยกฎเกณฑ์และ regular expression มีข้อดีคือ เป็นวิธีที่สะดวก และรันได้อย่างรวดเร็ว เพราะมีความซับซ้อนน้อย เหมาะสำหรับการวิเคราะห์ข้อมูลเบื้องต้น ที่ไม่ได้จำเป็นต้องสนใจเรื่องคำที่มีขีดคั่น รูปย่อ หรือตัวเลข และสามารถประยุกต์ใช้ได้กับภาษาหลัก ๆ ในโลกได้หลายหลายภาษา เช่น ภาษาอังกฤษ ภาษาสเปน ภาษาเยอรมัน ภาษารัสเซีย ภาษาอาหรับ ภาษาฮีบรู แต่ข้อเสียของวิธีนี้ คือ ไม่สามารถใช้กับภาษาที่ไม่มีการใช้ช่องว่างในการแบ่งคำ เช่น ภาษาไทย ภาษาจีน ภาษาญี่ปุ่น ภาษาเกาหลี


### การตัดคำโดยอาศัยคลังศัพท์ (lexicon-based word segmentation)
การตัดคำโดยอาศัยคลังศัพท์ เป็นวิธีการตัดคำที่มีประสิทธิภาพสูงวิธีหนึ่ง และมีหลักการคล้ายคลึงกับการเขียน regular expression เพื่อบ่งบอกว่าโทเค็นควรจะหน้าเป็นอย่างไร แต่ว่าเราจะระบุออกมาทั้งหมดเลยว่าอะไรบ้างที่ควรจะเป็นคำ (โทเค็น) ได้ เพราะฉะนั้นเราจะต้องมีลิสต์ของสตริงของภาษาที่เราต้องการจะตัดให้เป็นคำ เราเรียกลิสต์ของสตริงนั้นว่าคลังศัพท์ (lexicon) และใช้อัลกอริทึมในการไล่หาจากซ้ายไปขวาจนเจอคำที่ปรากฏอยู่ในคลังศัพท์ เช่น
*ไม่ต้องกลัวซ้ำกับใคร* กลายเป็น *ไม่|ต้อง|กลัว|ซ้ำ|กับ|ใคร* ได้เพราะเราลองไล่จากซ้ายไปขวาจนเจอคำว่า *ไม่* และไม่มีคำไหนเลยที่ขึ้นต้นด้วย *ไม่ต* จึงตัดคำว่า *ไม่* ออกมาได้ จากนั้นจึงไล่ต่อจนเจอคำว่า *ต้อง* และไม่มีคำไหนในคลังศัพท์เลยที่ขึ้นต้นด้วย *ต้องก* เราจึงตัด *ต้อง* ออกมา และทำเช่นเดียวกันแบบนี้ไปเรื่อย ๆ จนจบสตริง แล้วจะเห็นได้ว่าวิธีนี้ทำให้ทุกโทเค็นเป็นคำที่อยู่ในคลังศัพท์

```{margin} คำศัพท์
คลังศัพท์ (lexicon) คือ ลิสต์ของคำศัพท์คล้ายกับพจนานุกรม แต่มุ่งเน้นไปที่คำศัพท์ที่ใช้จริงเพื่อให้เครื่องนำไปใช้เป็นส่วนหนึ่งของระบบ 
```
คลังศัพท์เป็นจะได้มาจากพจนานุกรมอิเล็กทรอนิกส์ซึ่งมักมีคำศัพท์ที่พบเห็นบ่อย ๆ และผู้จัดทำพจนานุกรมได้ตรวจสอบเป็นที่เรียบร้อยแล้ว  แต่อย่างไรก็ตามเราสามารถพบข้อมูลที่มีคำที่ไม่ได้อยู่ในคลังศัพท์ ไม่ว่าจะเป็นชื่อเฉพาะที่เป็นภาษาไทย และภาษาต่างประเทศ คำที่สะกดไม่เป็นมาตรฐาน (เช่น *อัลไล* *ทามมาย* *เส็ด*) คำที่สะกดผิดโดยไม่ได้ตั้งใจ สำหรับกรณีเหล่านี้เราใช้อัลกอริทึม maximal matching ในการรับมือ ซึ่งวิธีการทำงานของอัลกอริทึมและโครงสร้างข้อมูลที่ต้องใช้มีรายละเอียดค่อนข้างซับซ้อน และอยู่นอกเหนือขอบเขตของเนื้อหาของหนังสือเล่มนี้ เรามักจะใช้ไลบรารีที่มีประสิทธิภาพสูงโดยไม่ต้องเขียนโค้ดใช้อัลกอริทึมนี้ด้วยตัวเอง ซึ่งจะสาธิตวิธีการใช้ในบทต่อไป    

การตัดคำโดยอาศัยคลังศัพท์มีข้อดีหลายประการ การตัดคำด้วยวิธีนี้มีความแม่นยำค่อนข้างสูง ประมาณ 70%-80% ขึ้นอยู่กับข้อมูลที่ใช้ในการทดสอบ และรันได้เร็ว {cite}`chormai-etal-2020-syllable` นอกจากนั้นแล้วยังสามารถปรับแต่งให้เข้ากับข้อมูลได้ง่ายโดยการเปลี่ยนหรือเพิ่มคำศัพท์เข้าไปในคลังคำศัพท์ที่ใช้ในการตัดคำ เช่น สมมติว่าเราต้องการวิเคราะห์ข้อมูลที่มาจากสื่อสังคมออนไลน์ เราสามารถเพิ่มประสิทธิภาพโดยการเพิ่มคำศัพท์ที่เป็นคำแสลงนี่เป็นที่นิยมในช่วงเวลานั้น ถ้าหากว่าเราต้องการวิเคราะห์ข้อมูลที่เกี่ยวกับสินค้าต่างประเทศ เราสามารถเพิ่มชื่อแบรนด์ ชื่อรุ่นสินค้า เข้าไปในคลังศัพท์ ทำให้เครื่องทราบว่าคำเหล่านี้เป็นคำที่ต้องตรวจจับให้ได้ คลังศัพท์มักจะอยู่ในรูปของไฟล์ที่มีคำอยู่ในนั้น ทำให้ผู้ที่ไม่มีพื้นฐานด้านการเขียนโค้ดก็สามารถปรับแต่งการตัดคำได้ และที่สำคัญที่สุดคือ การตัดคำด้วยวิธีนี้สามารถใช้ได้กับภาษาทุกภาษาที่ไม่มีการใช้ช่องว่างในการแบ่งคำ เราสามารถพบเห็นวิธีนี้ในการวิเคราะห์ข้อมูลภาษา หรือการสร้างแอพพลิเคชันเกี่ยวกับภาษาที่ต้องประมวลผลภาษาไทย ภาษาจีน ภาษาญี่ปุ่น ภาษาเกาหลี ส่วนภาษาอื่น ๆ ที่มีการใช้ตัวลาติน หรือระบบการเขียนที่มีช่องว่างระหว่างคำไม่จำเป็นต้องใช้การตัดคำด้วยวิธีนี้

การตัดคำโดยอาศัยคลังศัพท์มีข้อเสีย คือ ผลลัพธ์มักจะผิดพลาดเมื่อข้อมูลมีชื่อเฉพาะและคำศัพท์ภาษาต่างประเทศที่ทับศัพท์เป็นภาษาไทย ชื่อเฉพาะมักจะไม่อยู่ในคลังศัพท์ และแทบจะเป็นไปไม่ได้เลยที่จะเพิ่มคำศัพท์เข้าสู่คลังศัพท์ให้เป็นปัจจุบันตลอดเวลา เช่น ชื่อของคนไทยหลายชื่อเป็นชื่อที่ใหม่ตามสมัยนิยม และอาจจะไม่เหมือนใครเลย หรือชื่อวงดนตรี ชื่อแอพพลิเคชัน ชื่อห้างสรรพสินค้า ชื่อสถานที่ ก็มักจะเป็นชื่อใหม่ ๆ ไม่ซ้ำใคร ทำให้ปรับคลังศัพท์ได้ยาก ไม่เหมือนกับศัพท์ทางการแพทย์ หรือศัพท์เทคนิคอื่น ๆ ที่เรามักจะสามารถหาแหล่งความรู้ เช่น หนังสือตำรา ที่รวบรวมคำศัพท์เฉพาะเหล่านี้ได้อยู่แล้ว และไม่เปลี่ยนแปลงเร็วเหมือนชื่อเฉพาะ ส่วนคำศัพท์ภาษาต่างประเทศที่ทับศัพท์เป็นภาษาไทยก็สามารถรับมือได้ยาก เนื่องจากโลกสมัยปัจจุบันมีการเชื่อมต่อกับชาติอื่น ๆ มากมาก ในหนึ่งภาษาอาจจะมีคำศัพท์จากภาษาอื่น ๆ เราสามารถแปลงคลังศัพท์ทุกภาษาในโลกมาทับศัพท์เป็นภาษาไทยได้ครบ ที่สำคัญกว่านั้นคือการทับศัพท์อาจจะไม่เป็นไปตามมาตรฐานใดมาตรฐานเดียว อาทิ 
- คำว่า *application* คนทั่วไปอาจจะทับศัพท์ได้หลากหลายแบบ ได้แก่ *แอพพลิเคชัน* *แอพพลิเคชั่น* *แอปปลิเคชั่น* 
- คำว่า *graphics* คนทั่วไปอาจจะทับศัพท์เป็น *กราฟฟิก* *กราฟฟิค* *กราฟิค* 
- คำว่า *clinic* คนทั่วไปอาจจะทับศัพท์เป็น *คลินิก* *คลินิค* *คลีนิค*

ผู้ที่วิเคราะห์มักจะไม่สามารถไปบอกผู้ให้ข้อมูลให้เขียนให้ถูกต้องตามหลัก ตามมาตรฐานที่ต้องการ เราต้องวิเคราะห์ข้อมูลภาษาที่ใช้กันจริง ๆ ไม่ใช่ภาษาที่ตรงตามมาตรฐานราชบัณฑิต หรือมาตรฐานอื่น ๆ    ดังนั้นถ้าเราพบว่าข้อมูลที่เราได้มาอาจจะมีชื่อเฉพาะ คำศัพท์เทคนิค หรือคำทับศัพท์ภาษาต่างประเทศที่ไม่ได้สะกดด้วยมาตรฐานเดียวกัน เราจำเป็นต้องเลือกใช้วิธีการตัดคำแบบอื่น 

### การตัดคำโดยอาศัยการเรียนรู้ของเครื่อง (machine-learning-based word segmentation)

การตัดคำโดยอาศัยการเรียนรู้ของเครื่อง คือ การตัดคำโดยการเรียนรู้จากข้อมูล ไม่มีการกำหนดแพทเทิร์นหรือคลังศัพท์โดยตรง การตัดคำวิธีนี้อาศัยแบบจำลองหรือโมเดล (model) ที่ถูกฝึกขึ้นมาจากชุดข้อมูลการตัดคำ อัลกอริทึมและเทคนิควิธีในการสร้างแบบจำลองโดยการเรียนรู้จากชุดข้อมูล เรียกว่า การเรียนรู้ของเครื่อง (machine learning) ซึ่งเป็นเทคนิคที่สำคัญที่สุดในการสร้างปัญญาประดิษฐ์ (artificial intelligence) แบบจำลองจะทำการเรียนรู้หาแพทเทิร์นของตัวอักษรที่มักจะมาประกอบเป็นคำจากข้อมูลที่มีตัวอย่างการตัดคำที่ถูกต้อง กระบวนการนี้เรียกว่ากระบวนการฝึกแบบจำลอง (training) หลังจากที่โมเดลฝึกเสร็จเรียบร้อยแล้ว เราจึงนำโมเดลที่ได้ไปใช้ตัดคำจากข้อมูลที่ไม่เคยเห็นมาก่อน โมเดลประเภทนี้สามารถตรวจหาคำมักจะพบในคลังศัพท์ อีกทั้งยังสามารถขยายผลไปตรวจับแพทเทิร์นของคำที่อาจจะไม่ได้ปรากฏมาก่อนในชุดข้อมูล ไม่ว่าจะเป็นชื่อเฉพาะ หรือคำทับศัพท์ เนื่องจากทั้งชื่อเฉพาะ และคำทับศัพท์ ต่างก็มีแพทเทิร์นของมันเองที่ยากต่อการเขียน regular expression ออกมาให้ครบถ้วน

```{margin} คำศัพท์
การเรียนรู้ของเครื่อง (machine learning) คือ เทคนิคในการฝึกแบบจำลองหรือโมเดลให้ทำงานที่ต้องอาศัยความชาญฉลาด โดยการเรียนรู้จากข้อมูล

ปัญญาประดิษฐ์ (artificial intelligence) คือ โปรแกรมคอมพิวเตอร์ที่สามารถทำงานที่ต้องอาศัยความชาญฉลาดได้
```

ในการใช้งานจริงเรามักจะไม่สร้างโมเดลการตัดคำด้วยตัวเอง เรามักจะใช้ไลบรารีที่มีการควบรวมโมเดลที่ถูกฝึกมาก่อนหน้านี้แล้ว และนำมาใช้โดยไม่มีการปรับแต่ง นับเป็นวิธีที่ตัดคำได้แม่นยำที่สุด โดยมักจะได้ความแม่นยำประมาณ 85% ขึ้นอยู่กับชุดข้อมูลที่ใช้ในการทดสอบ {cite}`chormai-etal-2020-syllable,limkonchotiwat-etal-2020-domain,limkonchotiwat-etal-2021-handling` ดังนั้นการตัดคำโดยอาศัยการเรียนรู้ของเครื่องเป็นวิธีที่เหมาะสมที่สุดสำหรับการวิเคราะห์ข้อมูลตัวอักษรภาษาที่ไม่มีการใช้ช่องว่างในการแบ่งคำ 

การตัดคำโดยอาศัยการเรียนรู้ของเครื่องมีข้อเสียเรื่องความเร็ว หากใช้เครื่องคอมพิวเตอร์ทั่ว ๆ ไปมักจะใช้เวลาในการประมวลผลนานกว่าการตัดคำโดยใช้คลังศัพท์ถึง 5-10 เท่า โมเดลใช้เวลาประมาณ 10-20 วินาทีต่อการตัด 1 ล้านคำ {cite}`chormai-etal-2020-syllable` ถ้าหากเราต้องการวิเคราะห์ข้อมูลที่เราเก็บมาเสร็จเรียบร้อยแล้วและขนาดไม่ได้ใหญ่เกินไป เรายังสามารถใช้วิธีนี้ในการประมวลผลข้อมูลได้ เพราะเราสามารถแบ่งข้อมูลไปประมวลด้วยเครื่องคอมพิวเตอร์หลาย ๆ เครื่อง หรือใช้เครื่องที่มีกำลังในการคำนวณสูง ๆ แต่ถ้าหากเราต้องประมวลข้อมูลตามเวลาจริง (real-time) กล่าวคือเมื่อได้รับข้อมูลมาขณะนั้นแล้วต้องประมวลผลให้เสร็จในขณะนั้น การตัดคำด้วยวิธีนี้อาจจะช้าเกินไป นอกจากนั้นแล้ววิธีนีี้เป็นวิธีที่ปรับแต่งได้ยาก เพราะต้องเข้าใจวิธีการใช้การเรียนรู้ของเครื่องเพื่อฝึกโมเดลใหม่ หรือฝึกเพิ่มเติมจากโมเดลเดิม อีกทั้งยังต้องใช้ทรัพยากรในการสร้างชุดข้อมูลเพื่อฝึกโมเดลอีกด้วย 

### สรุปเรื่องการตัดคำ

การตัดคำทั้งสามวิธียังคงเป็นวิธีที่นิยมและมีประสิทธิภาพในการตัดคำ การเลือกใช้ตัวตัดคำต้องคำนึงถึงภาษาของข้อมูล และลักษณะของภาษาในชุดข้อมูล เราสามารถสรุปข้อดีและข้อเสียของตัดคำทั้งสามวิธีได้ดังนี้

| วิธีการตัดคำ | ส่วนประกอบที่สำคัญ | ข้อดี | ข้อเสีย |
|----------------|----------------------|-------|--------|
| การตัดคำโดยอาศัยกฎเกณฑ์  | กฎเกณฑ์ในรูป regular expression | เรียบง่าย และแม่นยำมาก | ใช้ได้กับเฉพาะภาษาที่ใช้ช่องว่างในการแบ่งคำ |
| ตัดคำโดยอาศัยคลังศัพท์ | คลังศัพท์ | แม่นยำ เร็ว และสามารถปรับแต่งได้ | มีปัญหาเรื่องคำที่ไม่อยู่ในคลังศัพท์ได้ |
| ตัดคำโดยอาศัยการเรียนรู้ของเครื่อง | โมเดลที่ถูกฝึกแล้ว | แม่นยำที่สุดสำหรับภาษาที่ไม่ใช้ช่องว่างในการแบ่งคำ  | ช้า ปรับแต่งได้ยาก |


## การวิเคราะห์ความถี่ของคำ (word frequency analysis)

เมื่อข้อมูลได้รับการทำความสะอาดและการแบ่งคำอย่างเหมาะสมแล้ว การวิเคราะห์ความถี่ของคำ เป็นการวิเคราะห์ขั้นพื้นฐานที่ช่วยให้เราเข้าใจเนื้อหาของชุดของเอกสารที่มีขนาดใหญ่ได้  ความถี่ของคำในการวิเคราะห์ข้อความ หมายถึง การนับจำนวนครั้งที่คำปรากฏในชุดข้อมูล การวิเคราะห์นี้เป็นพื้นฐานของหลายเทคนิคในการประมวลผลภาษาธรรมชาติ และมีความสำคัญในการเข้าใจลักษณะสำคัญของข้อมูลตัวอักษร ความถี่ของคำช่วยให้เราเห็นภาพรวมของหัวข้อหรือเนื้อหาที่ถูกพูดถึงบ่อยครั้งในชุดของเอกสารที่มีขนาดใหญ่ เพื่อให้เห็นภาพชัดขึ้น ลองพิจารณาแผนภูมิที่ {numref}`airline-review-word-freq`
    
```{figure} img/airline-review-word-freq.png
---
name: airline-review-word-freq
---

แผนภูมิแท่งแสดงความถี่ของคำจากชุดข้อมูล 30 อันดับแรก
```
้้เราพอจะเดาได้ว่าชุดข้อมูลที่นำมาวิเคราะห์นั้นเกี่ยวกับสายการบิน เพราะพบคำว่า *flight* *service* *airline* ด้วยความถี่รวมกว่า 50,000 ครั้ง รวมถึงพบคำอื่น ๆ ที่เกี่ยวกับสายการบินอยู่ใน 30 อันดับแรก เช่น *airport* *plane* *seats* *crew* *luggage* 

การวิเคราะห์ความถี่ของคำจากข้อมูลที่ได้มาจากลูกค้าช่วยให้เราเข้าใจได้ว่าลูกค้าพูดถึงสินค้าหรือบริการของเราด้วยคำใดบ่อยครั้งที่สุด ซึ่งสามารถชี้วัดได้ถึงปัจจัยที่ลูกค้าพอใจหรือไม่พอใจ หรือปัจจัยใดบ้างที่ลูกค้าให้ความสนใจ  การวิเคราะห์ข้อมูลรีวิว หรือข้อมูลบนสื่อสังคมออนไลน์ในลักษณะนี้ช่วยให้ธุรกิจสามารถปรับปรุงสินค้าหรือบริการของตนเองให้ตอบสนองความต้องการของลูกค้าได้ดียิ่งขึ้น 

ในทำนองเดียวกันเราสามารถใช้การวิเคราะห์ความถี่ของคำในลักษณะนี้เพื่อเปรียบเทียบเนื้อหาของชุดข้อมูลหลาย ๆ ชุดได้อีกด้วย เช่น หากเราต้องการวิเคราะห์เนื้อหา และความแตกต่างของหนังสือพิมพ์จาก 2 สำนักพิมพ์ที่อาจจะมีความเห็นต่างกัน เราสามารถคำนวณและเปรียบเทียบความถี่ของคำที่พบจากข้อมูลหนังสือพิมพ์จาก 2 สำนักพิมพ์นี้ 

การประยุกต์ใช้การวิเคราะห์ความถี่ของคำที่พบเห็นได้บ่อยที่สุด และเป็นที่นิยมมากขึ้นในขณะนี้ คือ ระบบ social listening ซึ่งเป็นระบบการติดตามและวิเคราะห์ข้อมูลจากสื่อสังคมออนไลน์และอินเทอร์เน็ตเพื่อเข้าใจถึงสิ่งที่กลุ่มเป้าหมายหรือผู้บริโภคกำลังพูดถึงแบรนด์ สินค้า บริการ หรือประเด็นที่เกี่ยวข้อง โดยเฉพาะอย่างยิ่งการตอบสนองต่อแคมเปญการตลาดหรือข่าวสารต่างๆ วิธีการนี้ช่วยให้องค์กรสามารถเก็บรวบรวมและวิเคราะห์ข้อมูลจากโซเชียลมีเดียเพื่อเข้าใจและตอบสนองต่อความต้องการและความคาดหวังของผู้บริโภคได้ดียิ่งขึ้น 

นอกจากนั้นแล้วการวิเคราะห์ความถี่ยังช่วยเน้นคำที่มีความสำคัญและสามารถเป็นตัวชี้วัดเชิงลึกเกี่ยวกับความรู้สึกและความคิดเห็นของผู้เขียนข้อความ ในหลายๆ กรณีการวิเคราะห์ความถี่ของคำเป็นขั้นตอนแรกที่นำไปสู่การวิเคราะห์ที่ซับซ้อนมากขึ้น เช่น การวิเคราะห์อารมณ์และความรู้สึก (sentiment analysis) หรือการสร้างโมเดลทางสถิติเพื่อทำนายพฤติกรรมของผู้ใช้หรือลูกค้า

### วิธีการวิเคราะห์ความถี่ของคำ

การคำนวณความถี่ของคำเป็นกระบวนการที่ไม่ซับซ้อน แต่ต้องอาศัยความละเอียดในการทำความสะอาดข้อมูลเพื่อให้ได้ผลลัพธ์ที่แม่นยำ  ขั้นตอนแรกในการคำนวณความถี่ของคำคือการเตรียมข้อมูลข้อความให้พร้อมสำหรับการวิเคราะห์ การเตรียมข้อมูลอาจรวมถึงการทำความสะอาดข้อมูล เช่น การลบอักขระพิเศษ แฮชแท็ก URL วันที่ หรือข้อมูล metadata อื่น ๆ ตามที่ได้อธิบายไปในบทนี้ นอกจากนั้นแล้วเราต้องกรองเอาคำที่ไม่เกี่ยวข้องกับการวิเคราะห์ออกไปอีกด้วย เช่น คำหยุด (stopword) ซึ่งเป็นคำที่มีความถี่สูงแต่ไม่มีความหมายในบริบทของการวิเคราะห์ ตัวอย่างของคำหยุด ได้แก่ "และ", "ที่", "ใน" เป็นต้น หลังจากนั้น คำในข้อมูลข้อความจะถูกแบ่งออกเป็นหน่วยย่อย เพื่อทำการวิเคราะห์ได้ง่ายขึ้น 

```{margin} คำศัพท์
คำหยุด (stopword) คือ คำที่ปรากฏบ่อยครั้งในภาษาธรรมชาติ แต่มีนัยสำคัญหรือความหมายเชิงเนื้อหาสาระน้อย 
```

```{figure} img/airline-review-with-stopwords.png
---
name: airline-review-with-stopwords
---

เมื่อข้อมูลได้รับการเตรียมพร้อมแล้ว ขั้นตอนถัดไปคือการนับจำนวนครั้งที่แต่ละคำปรากฏในชุดข้อมูล การนับนี้สามารถทำได้โดยใช้โปรแกรมหรือสคริปต์ที่เขียนขึ้นเพื่อส่งผ่านข้อมูลและทำการนับ ทุกครั้งที่พบคำที่ถูกกำหนด จะมีการเพิ่มค่านับของคำนั้น ๆ หลังจากการทำงานของสคริปต์เสร็จสิ้น ผลลัพธ์จะเป็นรายการของคำพร้อมกับจำนวนครั้งที่พบในข้อมูล ซึ่งเรียกว่า "ความถี่" ของคำนั้น ๆ ความถี่ของคำสามารถนำมาใช้ในการวิเคราะห์เพื่อเห็นแนวโน้มหรือลักษณะเด่นของข้อมูล


กฎของซิปฟ์ (Zipf's Law) เป็นหนึ่งในหลักการพื้นฐานในศาสตร์ของความถี่ของคำ ซึ่งกล่าวว่าคำที่ปรากฏในชุดข้อมูลข้อความจะมีความถี่ที่ลดลงอย่างรวดเร็วตามลำดับของความถี่นั้น นั่นคือคำที่มีความถี่สูงสุดจะมีจำนวนมากกว่าคำที่มีความถี่รองลงมาอย่างมาก และคำที่มีความถี่ต่ำสุดจะมีจำนวนน้อยมาก กฎนี้ช่วยให้เราเข้าใจการกระจายของความถี่ของคำในภาษาธรรมชาติและมีผลกระทบอย่างมากต่อการออกแบบและการประเมินผลของเทคนิคการวิเคราะห์ข้อความ การรู้จักกฎของซิปฟ์ช่วยให้นักวิเคราะห์ข้อมูลและนักพัฒนา NLP สามารถปรับใช้และพัฒนาเทคนิคการวิเคราะห์ข้อความให้เหมาะสมกับลักษณะของข้อมูล

ต้องกรอง stopwords ออก เพราะ Zipf's Law
และต้องวิเคราะห์คำหลาย ๆ ลำดับ เพราะว่าหางยาวตาม Zipf's Law 

การวิเคราะห์ความถี่ของคำไม่จำเป็นต้องจำกัดอยู่เพียงแค่การนับคำเท่านั้น แต่ยังสามารถขยายไปถึงการวิเคราะห์ความสัมพันธ์ระหว่างคำ การวิเคราะห์ความถี่ของคำสามารถใช้เทคนิคทางสถิติเพื่อหาความสัมพันธ์หรือแพทเทิร์นที่ซ่อนอยู่ในข้อมูลข้อความ ตัวอย่างเช่น การใช้การวิเคราะห์ความถี่ของคำร่วมกัน (Co-occurrence Analysis) ซึ่งเป็นวิธีที่สำรวจความสัมพันธ์ระหว่างคำที่ปรากฏขึ้นพร้อมกันในบริบทเดียวกัน สามารถเปิดเผยแนวคิดหรือหัวข้อที่เกี่ยวข้องซึ่งอาจไม่ชัดเจนเมื่อมองจากความถี่ของคำเพียงอย่างเดียว ด้วยวิธีนี้ การวิเคราะห์ความถี่ของคำไม่เพียงแต่เป็นเครื่องมือในการระบุคำที่มีความสำคัญหรือถูกพูดถึงบ่อยครั้งเท่านั้น แต่ยังเป็นวิธีในการสำรวจความซับซ้อนและความร่ำรวยของข้อมูลข้อความอีกด้วย


การกรองและตีความคำที่มีความถี่สูงเพื่อระบุธีมหรือหัวข้อทั่วไปในรีวิวเป็นกระบวนการที่สำคัญในการวิเคราะห์ข้อมูลข้อความ โดยเฉพาะอย่างยิ่งในธุรกิจที่ต้องการเข้าใจความคิดเห็นและความต้องการของลูกค้า เริ่มต้นด้วยการกรองคำหยุดและคำทั่วไปที่ไม่เพิ่มคุณค่าในการวิเคราะห์ อย่างไรก็ตาม การกรองคำเหล่านี้ต้องทำอย่างรอบคอบเพื่อไม่ให้สูญเสียข้อมูลที่สำคัญ หลังจากนั้น การใช้เทคนิคการวิเคราะห์เชิงสัมพันธ์ เช่น การวิเคราะห์ความถี่ของคำร่วม (co-occurrence analysis) สามารถช่วยให้เราเข้าใจความสัมพันธ์ระหว่างคำและวิธีที่คำเหล่านี้ถูกใช้ในบริบทที่แตกต่างกัน ซึ่งจะนำไปสู่การระบุธีมหรือหัวข้อที่ปรากฏอยู่ในรีวิวได้อย่างมีประสิทธิภาพ

การใช้เทคนิคการแสดงผลเช่น การสร้างคลาวด์คำ (word clouds) หรือการใช้กราฟฟิกและแผนภูมิเพื่อแสดงความถี่ของคำเป็นวิธีที่ดีในการทำให้ข้อมูลเข้าถึงได้ง่ายและเข้าใจได้ง่ายขึ้น คลาวด์คำเป็นเครื่องมือที่ยอดเยี่ยมในการแสดงความถี่ของคำผ่านขนาดของตัวอักษร ซึ่งคำที่มีความถี่สูงจะถูกแสดงด้วยตัวอักษรขนาดใหญ่กว่าคำที่มีความถี่น้อย วิธีนี้ช่วยให้ผู้วิเคราะห์สามารถจับภาพรวมของข้อมูลและระบุคำสำคัญที่อาจต้องการการวิเคราะห์เพิ่มเติมได้อย่างรวดเร็ว นอกจากนี้ การใช้แผนภูมิและกราฟฟิกอื่นๆ เช่น แผนภูมิบาร์หรือแผนภูมิเส้นสามารถช่วยในการแสดงความถี่ของคำและความสัมพันธ์ระหว่างคำในวิธีที่มีความหมายและเข้าใจง่าย

การวิเคราะห์ความถี่ของคำและการใช้เทคนิคการแสดงผลที่หลากหลายสามารถนำไปสู่การค้นพบความเชื่อมโยงและธีมที่ไม่ชัดเจนในการอ่านครั้งแรก โดยเฉพาะอย่างยิ่งในรีวิวทางธุรกิจ การวิเคราะห์เหล่านี้สามารถเผยให้เห็นถึงความต้องการและความคาดหวังของลูกค้า รวมถึงความพึงพอใจหรือความไม่พอใจต่อสินค้าหรือบริการ การใช้ข้อมูลนี้ช่วยให้ธุรกิจสามารถปรับปรุงผลิตภัณฑ์และบริการของตนเองได้อย่างมีเป้าหมาย และสามารถปรับแต่งการสื่อสารและกลยุทธ์การตลาดให้ตอบสนองความต้องการของลูกค้าได้อย่างเฉพาะเจาะจง ด้วยวิธีนี้ การวิเคราะห์ความถี่ของคำไม่เพียงช่วยในการทำความเข้าใจข้อมูลข้อความเท่านั้น แต่ยังเป็นกลยุทธ์ที่สำคัญในการนำธุรกิจไปสู่ความสำเร็จในยุคดิจิทัล


การวิเคราะห์ความถี่ของคำในบริบทของคำถามทางธุรกิจเฉพาะ เช่น คุณสมบัติของผลิตภัณฑ์, ความพึงพอใจของลูกค้า, และคุณภาพของบริการ, สามารถให้ข้อมูลที่มีค่าและเป็นประโยชน์สำหรับการตัดสินใจทางธุรกิจ การวิเคราะห์นี้ช่วยให้ธุรกิจสามารถระบุคำหรือวลีที่ลูกค้าใช้บ่อยครั้งเมื่อพูดถึงสินค้าหรือบริการของตน เช่น คำที่เกี่ยวข้องกับคุณสมบัติเฉพาะของผลิตภัณฑ์หรือบริการที่ลูกค้าพอใจหรือไม่พอใจ การวิเคราะห์เหล่านี้สามารถช่วยให้ธุรกิจประเมินความสำคัญของแต่ละคุณสมบัติและปรับปรุงหรือพัฒนาผลิตภัณฑ์ของตนให้ตรงกับความต้องการและความคาดหวังของลูกค้าได้มากขึ้น

การระบุคำที่แสดงความรู้สึกหรืออารมณ์เชิงบวกและเชิงลบ รวมถึงการวิเคราะห์ผลกระทบของคำเหล่านี้ต่อชื่อเสียงและยอดขายของธุรกิจเป็นส่วนสำคัญของการวิเคราะห์ข้อมูลรีวิว คำที่มีความรู้สึกเชิงบวกเช่น "พอใจ", "ยอดเยี่ยม", หรือ "เหนือความคาดหมาย" สามารถช่วยเน้นย้ำถึงจุดแข็งและคุณลักษณะที่ลูกค้าชื่นชอบ ในขณะที่คำเชิงลบเช่น "ผิดหวัง", "แย่", หรือ "ไม่น่าเชื่อถือ" สามารถชี้ให้เห็นถึงปัญหาหรือจุดอ่อนที่อาจต้องการการแก้ไข การวิเคราะห์คำเหล่านี้ไม่เพียงแต่ช่วยให้ธุรกิจเข้าใจถึงความรู้สึกและความคิดเห็นของลูกค้า แต่ยังช่วยให้สามารถเข้าถึงแนวทางในการปรับปรุงชื่อเสียงและเพิ่มยอดขาย

การใช้การวิเคราะห์ความถี่ของคำเพื่อติดตามการเปลี่ยนแปลงตามเวลาหรือตอบสนองต่อการดำเนินการหรือการแทรกแซงเฉพาะ เช่น การเปิดตัวผลิตภัณฑ์ใหม่หรือการเปลี่ยนแปลงในการให้บริการ เป็นวิธีที่มีค่าในการประเมินผลกระทบและประสิทธิผลของกลยุทธ์ที่บริษัทใช้ การวิเคราะห์เหล่านี้สามารถช่วยระบุแนวโน้มและการเปลี่ยนแปลงในความคิดเห็นของลูกค้าต่อเวลา เช่น การเพิ่มขึ้นหรือลดลงของความพึงพอใจหลังจากการเปิดตัวผลิตภัณฑ์ใหม่หรือการปรับปรุงบริการ การติดตามการเปลี่ยนแปลงเหล่านี้ช่วยให้ธุรกิจสามารถประเมินความสำเร็จของกลยุทธ์และทำการปรับเปลี่ยนหรือตัดสินใจทางธุรกิจอย่างมีข้อมูลเชิงลึก

การรวมการวิเคราะห์ความถี่ของคำเข้ากับการวิเคราะห์ความรู้สึกและการวิเคราะห์เชิงสัมพันธ์สามารถเพิ่มความเข้าใจและความแม่นยำในการระบุความต้องการและความคาดหวังของลูกค้า การวิเคราะห์ที่รวมกันเหล่านี้ช่วยให้ธุรกิจสามารถสร้างภาพรวมที่ครอบคลุมเกี่ยวกับการตอบรับของลูกค้าต่อสินค้าหรือบริการ นอกจากนี้ การเปรียบเทียบความถี่ของคำและความรู้สึกในหมวดหมู่หรือช่วงเวลาต่างๆ สามารถเปิดเผยข้อมูลเชิงลึกเกี่ยวกับแนวโน้มที่เกิดขึ้นใหม่หรือการเปลี่ยนแปลงในพฤติกรรมหรือความคาดหวังของลูกค้า

ในท้ายที่สุด การใช้การวิเคราะห์ความถี่ของคำในการเชื่อมโยงกับคำถามทางธุรกิจเฉพาะไม่เพียงแต่ช่วยให้ธุรกิจสามารถตอบสนองต่อความต้องการของลูกค้าได้ดีขึ้น แต่ยังช่วยให้ธุรกิจสามารถนำไปสู่การพัฒนาผลิตภัณฑ์และบริการที่เหนือกว่า การเข้าใจลึกซึ้งเกี่ยวกับวิธีที่ลูกค้าพูดถึงผลิตภัณฑ์และบริการของธุรกิจ รวมถึงการรับรู้ชื่อเสียงของแบรนด์ในตลาด สามารถนำไปสู่การตัดสินใจและการดำเนินการทางธุรกิจที่ขับเคลื่อนด้วยข้อมูล ซึ่งไม่เพียงแต่เพิ่มยอดขายและชื่อเสียงเท่านั้น แต่ยังเพิ่มความภักดีและความพึงพอใจของลูกค้าในระยะยาว


การวิเคราะห์ความถี่ของคำเป็นเครื่องมือที่มีประโยชน์ในการประมวลผลและการวิเคราะห์ข้อมูลข้อความในหลากหลายบริบท แต่ก็มีข้อจำกัดที่สำคัญที่ต้องคำนึงถึง หนึ่งในข้อจำกัดหลักคือการวิเคราะห์ความถี่ของคำไม่สามารถระบุความหมายและบริบทของคำในข้อความได้อย่างละเอียด ความถี่ของคำบอกเราได้เพียงว่าคำใดปรากฏบ่อยครั้งในชุดข้อมูล แต่ไม่สามารถบอกความหมายหรือน้ำหนักของคำนั้นในบริบทที่แตกต่างกันได้ ตัวอย่างเช่น คำที่มีความหมายหลายแง่มุมอาจถูกนับเป็นคำเดียวกันโดยไม่คำนึงถึงความแตกต่างในการใช้งาน ซึ่งสามารถนำไปสู่การตีความที่ไม่แม่นยำของข้อมูล

อีกข้อจำกัดคือการวิเคราะห์ความถี่ของคำอาจไม่สามารถจับคู่ความซับซ้อนและระดับความหมายที่ซ่อนอยู่ในข้อความได้ การใช้คำในบริบทที่มีความหมายเชิงลบหรือเชิงบวกอาจไม่ได้ถูกแสดงออกโดยความถี่ของคำเหล่านั้นโดยตรง ตัวอย่างเช่น การวิจารณ์สินค้าหรือบริการอย่างเห็นแก่ดีอาจมีการใช้คำที่มีความถี่สูงในเชิงบวก แต่เมื่อวิเคราะห์ในบริบทจะเห็นว่ามีความหมายเชิงลบ การขาดการพิจารณาความหมายและบริบทสามารถนำไปสู่การตีความข้อมูลที่ผิดพลาดและการตัดสินใจที่ไม่ถูกต้อง

ข้อจำกัดที่สามคือการวิเคราะห์ความถี่ของคำไม่สามารถจับคู่กับความสัมพันธ์ระหว่างคำและความหมายที่เกิดขึ้นจากการใช้คำในบริบทที่แตกต่างกัน การเข้าใจความสัมพันธ์เชิงซ้อนระหว่างคำ (เช่น คำที่มีความหมายเมื่อใช้ร่วมกันในประโยคหรือวลี) และการประเมินความสำคัญของคำเหล่านี้ต่อข้อความโดยรวมต้องการเทคนิคการวิเคราะห์ที่ซับซ้อนกว่าการนับความถี่เพียงอย่างเดียว การวิเคราะห์ความถี่อาจไม่เพียงพอในการตรวจจับและตีความความซับซ้อนเหล่านี้ได้อย่างแม่นยำ ซึ่งจำเป็นต้องใช้เทคนิคเช่นการวิเคราะห์ความสัมพันธ์ของคำและการวิเคราะห์ความหมายเชิงลึกเพื่อเข้าถึงความเข้าใจที่ครอบคลุมและลึกซึ้งยิ่งขึ้น



```{bibliography}
:style: plain
```